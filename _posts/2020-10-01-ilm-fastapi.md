---
title: Hosting the Infilling Model through a FastAPI on GCP
tags: [Coding]
style: fill
color: danger
description: BERT masking does not allow to infill multiple words into a sentence context. Researchers from Stanford addressed this. I made it available through an API.
---

This article is based on the works of the Infilling Language Model (ILM) framework outlined in the ACL 2020 paper [Enabling language models to fill in the blanks](https://arxiv.org/abs/2005.05339) by Donahue et al. 2020. Following these instructions allows to generate infillings into a specific context by simply adding a '_'-token. The infilling can be done with words, ngrams (multiple words), or sentences. Find an example generated by the Deep Neural Network GPT-2 below:

![Imgur](https://i.imgur.com/Qh0FFjN.png)

The Git that produces this infilling output can be found [here](https://github.com/seduerr91/ilm-api). 
The instructions for deployment as a micro service on Google Cloud Engine can be found below.

## What is infilling?

Text infilling is the task of predicting missing spans of text which are consistent with the preceding and subsequent text.

## The Task: Making ILM Available Through as a Micros Service

In order to use this service in production it needs to be made availble through an API for which we use FastAPI. The resulting API can be tested [here](https://ilmapi.uc.r.appspot.com/docs) and renders as depicted below:

![Imgur](https://i.imgur.com/kbHNMpM.png)

## Running the Infilling Model on Google Cloud Platform by Yourself

1. In order to run this please create a new project on [Google Cloud Platform](https://cloud.google.com/). For the new project being created, give Project name, Project ID, Billing account and click Create.

![Imgur](https://i.imgur.com/tTvOugf.png)

2. Activate the Cloud Shell in the upper right corner to execute a few lines of code in order to host the ILM-API.

![Imgur](https://i.imgur.com/IHxxlJu.png)

3. Next, clone the [ILM-API Repository](https://github.com/seduerr91/ilm-api) from GitHub. Type the following command in the Cloud Shell to clone the FastAPI repository on to GCP: 

    git clone https://github.com/seduerr91/ilm-api.git

[Optionally], you now can create a virtual environment. Find instructions [here](https://docs.python.org/3/tutorial/venv.html).

4. Install Requirements for the FastAPI and the ILM. The project related libraries are located in requirements.txt. To install all the modules in one go, run the following command:

    pip3 install -r requirements.txt

5. To deploy the FastAPI app on App Engine and access it via a custom domain or default your-proj-id.appspot.com, we need to create gcloud app and deploy our app. You will be prompted to select the region. It is always a best practice to choose the region nearest to the geographical location where the consumers access this application. Doing so is expected increase the response times from our application. In the cloud shell, type the following command to create the app:

    gcloud app create

6. Deploy FastAPI app on GCP App Engine. GCP App Engine requires all the deployment configuration to be defined in a yaml file. Our repository has already has a yaml file named app.yaml that has the deployment configuration defined. Run the following command in Google Cloud Shell to deploy the FastAPI app to Google App Engine. You will be prompted to continue. Press Y and hit enter to proceed with the deployment.

    gcloud app deploy app.yaml

7. Browse the FastAPI deployed on GCP App Engine. Generally your app is deployed on the url that has the following format. your-project-id.appspot.com. In case you are not sure what the project id is, then type the following command to view your application in the web browser.

    gcloud app browse

#### Congratulations: You just hosted the Infilling Model (Donahue et al. 2020) based on the Deep Neural Architecture GPT-2 via FastAPI as a microservice.

[Optional] <!-- Reload Server with new features -->


<!-- Get the link to the API -->


<!-- Get logs -->
gcloud app logs tail -s default

## Some Words About the Code Files

### Infill.py

In order to have a lightweight infilling language model, all libraries, files and scripts were removed that are not related to inference. The resulting inference method was then wrapped into the 'class Infiller()' which we need for the API in 'main.py'. Fin the infill.py below.

<script src="https://gist.github.com/seduerr91/9183c728c18461c98c2f8ab5b9517009.js"></script>

### The main.py

We serve the uvicorn server through the main.py file with 'uvicorn main:app'. It does...

<script src="https://gist.github.com/seduerr91/e389a2c212452f459c37346530a388b0.js"></script>

### Requirements.txt

The requirements.txt looks liek this. You run it via 'pip3 install -r requirements.txt'. It does not take much.

<script src="https://gist.github.com/seduerr91/60ae1fdc383ece9daa5007f3a180240e.js"></script>

### YAML Configuration to deploy FastAPI on Google App Engine
Google Cloud Platform allows App Engine to perform deployment based on a configuration defined in a yaml file. For us to host the FastAPI on Google App Engine, the yaml configuration needs to have the following configuration.

<script src="https://gist.github.com/seduerr91/2fcd135a83023cbcfefb66b373b9ec58.js"></script>

### The Dockerfile

The Dockerfile is being run through the app.yaml

<script src="https://gist.github.com/seduerr91/5cdbd83bd095a421120e06d209d7fe24.js"></script>

Following instructions will allow you to run the [files](https://www.tutlinks.com/deploy-fastapi-app-on-google-cloud-platform/).

## Resources

- [Git Infilling by Language Modeling (ILM)](https://github.com/chrisdonahue/ilm)
- [HuggingFace Pipelines](https://huggingface.co/transformers/main_classes/pipelines.html)
- [Deploy FastAPI App on Google Cloud Platform](https://www.tutlinks.com/deploy-fastapi-app-on-google-cloud-platform/)
- [Build And Host Fast Data Science Applications Using FastAPI](https://towardsdatascience.com/build-and-host-fast-data-science-applications-using-fastapi-823be8a1d6a0)
- [Deploying Transformer Models](https://chatbotslife.com/deploying-transformer-models-1350876016f)
- [How to properly ship and deploy your machine learning model](https://towardsdatascience.com/how-to-properly-ship-and-deploy-your-machine-learning-model-8a8664b763c4)
- [Setting Up Virtual Environments](https://docs.python.org/3/tutorial/venv.html)

## Acknolegdements

Thank you to Javier and the team of Narrativa who suggested me to find a solution to this problem. Also a big thanks to HuggingFace, the Team from Stanford around Mr. Donahue, and my WifiTribe with which I am currently living as a Digital Nomad.

## Who am I?

I am Sebastian Duerr a Deep Learning Research Scientist. In my former life, I was a managing at Austria's biggest bank but I am diving into Deep Learning Architectures in the field of NLP. In the future, I want to work remotely whenever I want to in the field of NLP. 

Drop me a message on [linkedIn](https://www.linkedin.com/in/sebastianduerr/).